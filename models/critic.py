"""
Critic model for CNN-LSX.

The Critic evaluates the quality of explanations produced by the Learner.
It is trained to predict the target class from the explanations.
"""

from utils.common import colored_text
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch import Tensor
from torch.utils.data import DataLoader
from typing import Tuple, List, Optional, Callable, Dict, Any

# Import utilities
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class Critic:
    """
    The Critic evaluates explanation quality.

    The Critic is trained to predict the correct class from the explanations
    generated by the Learner. The performance of the Critic reflects the
    quality of the explanations - if they contain class-relevant information,
    the Critic will perform well.
    """

    def __init__(
        self,
        model: nn.Module,
        critic_loader: DataLoader,
        device: torch.device,
        class_weights: Optional[Tensor] = None,
        shuffle_data: bool = True,
        random_mode: bool = False,
        log_interval: Optional[int] = None
    ):
        """
        Initialize a Critic model.

        Args:
            model: Neural network model for the critic
            critic_loader: DataLoader for critic training data
            device: Device to use
            class_weights: Weights for different classes
            shuffle_data: Whether to shuffle data during training
            random_mode: If True, randomly permute labels (for baseline)
            log_interval: Interval for logging
        """
        self.classifier = model.to(device)
        self.critic_loader = critic_loader
        self.device = device
        self.shuffle_data = shuffle_data
        self.random_mode = random_mode
        self.log_interval = log_interval

        # Set up class weights for loss function
        if class_weights is not None:
            self.class_weights = class_weights.to(device)
        else:
            self.class_weights = torch.ones(model.fc2.out_features).to(device)

        # Initialize global step counter
        self.global_step = 0

    def train(
        self,
        explanations: List[Tensor],
        learning_rate: float,
        max_epochs: int = 1
    ) -> Tuple[float, float, float]:
        """
        Train the critic on the provided explanations.

        Args:
            explanations: List of explanation tensors
            learning_rate: Learning rate for optimizer
            max_epochs: Maximum number of epochs

        Returns:
            Tuple of (initial_loss, final_loss, mean_loss)
        """
        self.classifier.train()

        # If explanations are provided, zip with critic data and shuffle
        if explanations:
            data_pairs = list(zip(explanations, list(self.critic_loader)))
            if self.shuffle_data:
                shuffled_data_pairs = random.sample(
                    data_pairs, len(data_pairs))
            else:
                shuffled_data_pairs = data_pairs

            permuted_explanations, permuted_critic_set = zip(
                *shuffled_data_pairs)
        else:
            # If no explanations (e.g., for baseline), just use inputs
            permuted_explanations = []
            permuted_critic_set = list(self.critic_loader)
            if self.shuffle_data:
                random.shuffle(permuted_critic_set)

        # Set up loss function and optimizer
        criterion = nn.CrossEntropyLoss(self.class_weights)
        optimizer = optim.Adadelta(
            self.classifier.parameters(), lr=learning_rate)

        # Track losses
        losses = []

        # Train for specified number of epochs
        for epoch in range(max_epochs):
            # Train on each batch
            for n_batch, (inputs, labels) in enumerate(permuted_critic_set):
                # Move tensors to device
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                # Process batch
                loss = self._process_batch(
                    loss_function=criterion,
                    explanations=permuted_explanations,
                    inputs=inputs,
                    labels=labels,
                    n_current_batch=n_batch,
                    optimizer=optimizer
                )

                losses.append(loss)
                self.global_step += 1

        # Return statistics
        return losses[0], losses[-1], sum(losses) / len(losses)

    def _process_batch(
        self,
        loss_function: nn.Module,
        explanations: List[Tensor],
        inputs: Tensor,
        labels: Tensor,
        n_current_batch: int,
        optimizer: optim.Optimizer
    ) -> float:
        """
        Process a single batch during training.

        Args:
            loss_function: Loss function to use
            explanations: List of explanation tensors
            inputs: Input tensors
            labels: Target labels
            n_current_batch: Current batch number
            optimizer: Optimizer to use

        Returns:
            Loss value
        """
        optimizer.zero_grad()

        # Use explanations if provided, otherwise use inputs
        if explanations:
            explanation_batch = explanations[n_current_batch]
        else:
            explanation_batch = inputs

        # Forward pass
        outputs = self.classifier(explanation_batch)

        # If in random mode, shuffle labels (for baseline)
        if self.random_mode:
            idx = torch.randperm(labels.shape[0])
            labels = labels[idx]

        # Compute loss and backpropagate
        loss = loss_function(outputs, labels)
        loss.backward()
        optimizer.step()

        # Log results
        self._log_results(loss, n_current_batch)

        return loss.item()

    def _log_results(self, loss: Tensor, n_current_batch: int) -> None:
        """
        Log training results.

        Args:
            loss: Loss tensor
            n_current_batch: Current batch number
        """
        if self.log_interval and n_current_batch % self.log_interval == 0:
            print(
                f'Critic batch = {n_current_batch}, loss = {loss.item():.3f}')

    def evaluate(
        self,
        dataloader: DataLoader,
        use_explanations: bool = True,
        explanation_generator: Optional[Callable] = None
    ) -> float:
        """
        Evaluate the critic model.

        Args:
            dataloader: DataLoader for evaluation
            use_explanations: Whether to use explanations or raw inputs
            explanation_generator: Function to generate explanations

        Returns:
            Accuracy (0.0-1.0)
        """
        self.classifier.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in dataloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                # Use explanations if requested and generator is provided
                if use_explanations and explanation_generator:
                    batch_input = explanation_generator(inputs, labels)
                else:
                    batch_input = inputs

                # Forward pass
                outputs = self.classifier(batch_input)
                _, predicted = torch.max(outputs.data, 1)

                # Update statistics
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = correct / total
        self.classifier.train()

        return accuracy
